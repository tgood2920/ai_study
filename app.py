import streamlit as st
import os
import time
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate

# 1. í™˜ê²½ ì„¤ì •
load_dotenv()
st.set_page_config(page_title="ì½”ë”” ì„ ìƒë‹˜ (Live)", page_icon="ğŸ‘¨â€ğŸ«")
st.title("ğŸ‘¨â€ğŸ« ì‹¤ì‹œê°„ìœ¼ë¡œ ê°€ë¥´ì³ì£¼ëŠ” 'ì½”ë””'")

# 2. PDF ì²˜ë¦¬ ë° ë²¡í„° DB ìƒì„±
@st.cache_resource
def process_pdf(file_path):
    loader = PyPDFLoader(file_path)
    docs = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
    # í€´ì¦ˆ ìƒì„±ì„ ìœ„í•´ ê²€ìƒ‰ê¸°(retriever)ì™€ ì›ë³¸(docs) ë‘˜ ë‹¤ ë°˜í™˜
    return vectorstore.as_retriever(), docs

# 3. ìš”ì•½ ë° í€´ì¦ˆ ìƒì„±
def generate_summary_and_quiz(docs):
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7)
    
    # ì•ë¶€ë¶„ 3í˜ì´ì§€ë§Œ ì½ê¸°
    max_pages = 3
    context_text = "\n\n".join([doc.page_content for doc in docs[:max_pages]])

    prompt = f"""
    ë„ˆëŠ” ì´ˆë“±í•™ìƒë“¤ì˜ ëˆˆë†’ì´ì— ë§ì¶˜ ì¹œì ˆí•œ ì„ ìƒë‹˜ 'ì½”ë””'ì•¼. 
    ì•„ë˜ [êµì¬ ë‚´ìš©]ì„ ë³´ê³  ìˆ˜ì—…ì„ ì¤€ë¹„í•´ì¤˜.
    
    [êµì¬ ë‚´ìš©]:
    {context_text}
    
    [ìš”ì²­ ì‚¬í•­]:
    1. **ì˜¤ëŠ˜ì˜ í•™ìŠµ ëª©í‘œ**: í•µì‹¬ ì£¼ì œ 3ê°€ì§€ë¥¼ ë½‘ì•„ì„œ ìš”ì•½í•´ì¤˜.
    2. **ì¬ë¯¸ìˆëŠ” í€´ì¦ˆ**: ì•„ì´ë“¤ì´ í¥ë¯¸ë¥¼ ê°€ì§ˆë§Œí•œ ë‚´ìš©ìœ¼ë¡œ 3ì§€ ì„ ë‹¤í˜• í€´ì¦ˆë¥¼ 1ê°œë§Œ ë§Œë“¤ì–´ì¤˜.
    
    ì¶œë ¥ í˜•ì‹:
    ---
    ### ğŸ“ ì˜¤ëŠ˜ì˜ í•™ìŠµ ëª©í‘œ
    (ìš”ì•½)
    
    ### ğŸ§© íŒ í€´ì¦ˆ!
    (ë¬¸ì œ)
    1. (ë³´ê¸°)
    2. (ë³´ê¸°)
    3. (ë³´ê¸°)
    ---
    """
    response = llm.invoke(prompt)
    return response.content

# --- UI êµ¬ì„± ---
with st.sidebar:
    st.header("êµì¬ ì—…ë¡œë“œ ğŸ“¤")
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”", type=["pdf"])

if uploaded_file is not None:
    temp_pdf_path = "temp_lesson.pdf"
    with open(temp_pdf_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    
    try:
        if "retriever" not in st.session_state:
            with st.spinner("ì½”ë””ê°€ êµì¬ë¥¼ ì½ê³  ìˆ˜ì—… ì¤€ë¹„ ì¤‘... ğŸ“š"):
                retriever, docs = process_pdf(temp_pdf_path)
                st.session_state["retriever"] = retriever
                
                summary = generate_summary_and_quiz(docs)
                
                st.session_state["messages"] = [
                    AIMessage(content=f"ì, ìˆ˜ì—… ì‹œì‘í•´ë³¼ê¹Œìš”? ğŸ˜\n\n{summary}")
                ]
        st.success("ìˆ˜ì—… ì¤€ë¹„ ì™„ë£Œ!")
        
    except Exception as e:
        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()
else:
    if "retriever" in st.session_state:
        del st.session_state["retriever"]
        st.session_state["messages"] = []
    st.info("ğŸ‘ˆ ì™¼ìª½ì—ì„œ PDF êµì¬ë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    st.stop()

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for msg in st.session_state["messages"]:
    if isinstance(msg, HumanMessage):
        st.chat_message("user").write(msg.content)
    elif isinstance(msg, AIMessage):
        st.chat_message("assistant").write(msg.content)

# â˜… ì—¬ê¸°ê°€ í•µì‹¬: ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë° ìŠ¤íŠ¸ë¦¬ë°
if user_input := st.chat_input("ì§ˆë¬¸í•´ ë³´ì„¸ìš”!"):
    # 1. ì‚¬ìš©ì ë©”ì‹œì§€ í™”ë©´ í‘œì‹œ
    st.chat_message("user").write(user_input)
    st.session_state["messages"].append(HumanMessage(content=user_input))

    # 2. AI ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        # (1) ê²€ìƒ‰ ë‹¨ê³„
        with st.spinner("êµê³¼ì„œ ë’¤ì ì´ëŠ” ì¤‘... ğŸ“–"):
            retriever = st.session_state["retriever"]
            retrieved_docs = retriever.invoke(user_input)
            context = "\n\n".join([doc.page_content for doc in retrieved_docs])
        
        # (2) [New] ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸° (ìµœê·¼ 3ê°œë§Œ ê°€ì ¸ì™€ì„œ ê¸°ì–µë ¥ ì£¼ì…)
        # ë„ˆë¬´ ë§ì´ ê°€ì ¸ì˜¤ë©´ í† í° ë¹„ìš©ì´ ë“œë‹ˆ, ìµœê·¼ ëŒ€í™”(í€´ì¦ˆ ë‚¸ ê±°)ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        chat_history = []
        for msg in st.session_state["messages"][-3:]: 
            role = "AI ì„ ìƒë‹˜" if isinstance(msg, AIMessage) else "í•™ìƒ"
            chat_history.append(f"{role}: {msg.content}")
        history_text = "\n".join(chat_history)

        # (3) í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ëŒ€í™” ê¸°ë¡ í¬í•¨)
        prompt_template = ChatPromptTemplate.from_template(f"""
        ë„ˆëŠ” ì¹œì ˆí•œ ì„ ìƒë‹˜ 'ì½”ë””'ì•¼. 
        
        [ì§€ì‹œ ì‚¬í•­]:
        1. ì•„ë˜ [ëŒ€í™” ê¸°ë¡]ì„ ë³´ê³  íë¦„ì„ íŒŒì•…í•´. (ë„¤ê°€ ë°©ê¸ˆ í€´ì¦ˆë¥¼ ëƒˆë‹¤ë©´ ì •ë‹µì„ í™•ì¸í•´ì¤˜!)
        2. [ì°¸ê³  ìë£Œ]ì— ì—†ëŠ” ë‚´ìš©ì€ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  í•´.
        3. í•™ìƒì˜ ì§ˆë¬¸ì´ë‚˜ ëŒ€ë‹µì— ì¹œì ˆí•˜ê²Œ ë°˜ì‘í•´ì¤˜.

        [ëŒ€í™” ê¸°ë¡]:
        {{history}}

        [ì°¸ê³  ìë£Œ]:
        {{context}}
        
        í•™ìƒ ì§ˆë¬¸: {{input}}
        """)
        
        # (4) LLM í˜¸ì¶œ
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7)
        chain = prompt_template | llm
        
        # historyì™€ context, inputì„ ëª¨ë‘ ë„£ì–´ì¤ë‹ˆë‹¤.
        chunks = chain.stream({
            "history": history_text, 
            "context": context, 
            "input": user_input
        })
        
        # (5) ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
        for chunk in chunks:
            if chunk.content:
                full_response += chunk.content
                message_placeholder.markdown(full_response + "â–Œ")
                time.sleep(0.03) # íƒ€ì ì†ë„
        
        message_placeholder.markdown(full_response)
    
    # 3. ì„¸ì…˜ì— ì €ì¥
    st.session_state["messages"].append(AIMessage(content=full_response))