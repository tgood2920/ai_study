import streamlit as st
import os
import time
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate

# 1. í™˜ê²½ ì„¤ì •
load_dotenv()
st.set_page_config(page_title="RFP ì…ì°° ë¶„ì„ê¸° (Pro)", page_icon="ğŸ“‘", layout="wide")

# [ì¶”ê°€í•  ì½”ë“œ] â˜…â˜…â˜… ì—¬ê¸°ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤! â˜…â˜…â˜…
# ëŒ€í™” ê¸°ë¡ ì‚¬ë¬¼í•¨ì´ ì—†ìœ¼ë©´ ë¯¸ë¦¬ ë¹ˆ í†µì„ ë§Œë“¤ì–´ë‘¡ë‹ˆë‹¤.
if "messages" not in st.session_state:
    st.session_state["messages"] = []
    
st.title("ğŸ“‘ ì œì•ˆìš”ì²­ì„œ(RFP) í•µì‹¬ ë¶„ì„ê¸°")
st.markdown("ë³µì¡í•œ ê³µê³ ë¬¸, **30ì´ˆ ë§Œì— í•µì‹¬ë§Œ íŒŒì•…**í•˜ê³  **ë…ì†Œ ì¡°í•­**ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.")

# 2. PDF ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)
@st.cache_resource
def process_pdf(file_path):
    loader = PyPDFLoader(file_path)
    docs = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
    return vectorstore.as_retriever(), docs

# 3. [í•µì‹¬] RFP ë¶„ì„ ì „ë¬¸ í”„ë¡¬í”„íŠ¸
def analyze_rfp(docs, project_name):
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.3) # ë¶„ì„ì€ ì°½ì˜ì„±ë³´ë‹¤ ì •í™•ì„±ì´ ì¤‘ìš”í•˜ë¯€ë¡œ temperatureë¥¼ ë‚®ì¶¤
    
    # ì•ë¶€ë¶„(ê³µê³  ê°œìš”)ê³¼ ë’·ë¶€ë¶„(í‰ê°€ ê¸°ì¤€)ì„ ê³¨ê³ ë£¨ ë³´ê¸° ìœ„í•´ ì• 5í˜ì´ì§€ + ë’¤ 3í˜ì´ì§€ ì •ë„ë¥¼ ì¡°í•©í•˜ë©´ ì¢‹ì§€ë§Œ, 
    # ì¼ë‹¨ ì‹¬í”Œí•˜ê²Œ ì•ë¶€ë¶„ 5í˜ì´ì§€ë§Œ ì½ì–´ì„œ ê°œìš”ë¥¼ íŒŒì•…í•˜ê²Œ í•©ë‹ˆë‹¤. (ì „ì²´ ë¶„ì„ì€ RAGë¡œ ì§ˆë¬¸)
    max_pages = 5
    context_text = "\n\n".join([doc.page_content for doc in docs[:max_pages]])

    prompt = f"""
    ë„ˆëŠ” ê³µê³µì‚¬ì—… ì…ì°° ë° ì œì•ˆì„œ ì‘ì„± ì „ë¬¸ê°€(Senior PM)ì•¼.
    ë‚´ê°€ ì—…ë¡œë“œí•œ [ì œì•ˆìš”ì²­ì„œ(RFP)]ì˜ ì•ë¶€ë¶„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ í•­ëª©ë“¤ì„ ì•„ì£¼ ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì¤˜.
    
    [ë¶„ì„ ëŒ€ìƒ ì‚¬ì—…ëª…]: {project_name}
    
    [RFP ë‚´ìš© ì¼ë¶€]:
    {context_text}
    
    [ìš”ì²­ ì‚¬í•­ - ë°˜ë“œì‹œ ì•„ë˜ í¬ë§·ìœ¼ë¡œ ì¶œë ¥]:
    
    ## 1. ğŸ¯ ì‚¬ì—… ê°œìš” ìš”ì•½
    * **ì‚¬ì—… ëª©ì **: (í•œ ì¤„ ìš”ì•½)
    * **ì‚¬ì—… ì˜ˆì‚°**: (ê¸ˆì•¡ì´ ë³´ì´ë©´ ì ê³ , ì•ˆ ë³´ì´ë©´ 'ë¬¸ì„œ ë‚´ ê²€ìƒ‰ í•„ìš”'ë¼ê³  ì ìŒ)
    * **ì‚¬ì—… ê¸°ê°„**: (ê¸°ê°„ ëª…ì‹œ)
    * **ì£¼ìš” ê³¼ì—…**: (í•µì‹¬ ìš”êµ¬ì‚¬í•­ 3~5ê°€ì§€ ë¶ˆë › í¬ì¸íŠ¸)

    ## 2. âš ï¸ ë¦¬ìŠ¤í¬ ë° ì œì•½ì‚¬í•­ (ë…ì†Œ ì¡°í•­ ì²´í¬)
    * **ì…ì°° ìê²©**: (íŠ¹ì • ë¼ì´ì„ ìŠ¤ë‚˜ ì‹¤ì  ìš”êµ¬ê°€ ìˆëŠ”ì§€)
    * **ì¸ë ¥ ìš”ê±´**: (PMë“±ê¸‰, ìƒì£¼ ì—¬ë¶€ ë“± íŠ¹ì´ì‚¬í•­)
    * **íŒ¨ë„í‹°/ì œì•½**: (ì§€ì²´ìƒê¸ˆë¥ , ê¸°ìˆ ë£Œ ë“± ìœ„í—˜ ìš”ì†Œê°€ ë³´ì´ë©´ ê¸°ìˆ )

    ## 3. ğŸ“ ì œì•ˆì„œ ëª©ì°¨ ì¶”ì²œ (ì´ˆì•ˆ)
    (ì´ RFPì— ë§ì¶°ì„œ ìš°ë¦¬ê°€ ì‘ì„±í•´ì•¼ í•  ì œì•ˆì„œì˜ ëª©ì°¨(Index)ë¥¼ 1, 2, 3ë‹¨ê³„ë¡œ êµ¬ì„±í•´ì¤˜)
    
    ---
    **ğŸ’¡ Tip:** ë” ìì„¸í•œ ê¸°ìˆ  ìš”êµ¬ì‚¬í•­ì´ë‚˜ í‰ê°€ í•­ëª©ì€ ì±„íŒ…ì°½ì— ë¬¼ì–´ë³´ì‹œë©´ ì°¾ì•„ë“œë¦´ê²Œìš”!
    """
    response = llm.invoke(prompt)
    return response.content

# --- UI êµ¬ì„± ---
with st.sidebar:
    st.header("ğŸ“‚ ë¶„ì„ íŒŒì¼ ì—…ë¡œë“œ")
    
    project_name = st.text_input("ì‚¬ì—…ëª… (í”„ë¡œì íŠ¸ ì´ë¦„)", value="ì°¨ì„¸ëŒ€ ì •ë³´ì‹œìŠ¤í…œ êµ¬ì¶• ì‚¬ì—…")
    
    st.info("ğŸ’¡ HWP íŒŒì¼ì€ PDFë¡œ ë³€í™˜í•´ì„œ ì˜¬ë ¤ì£¼ì„¸ìš”.")
    uploaded_file = st.file_uploader("RFP(PDF) íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”", type=["pdf"])
    
    st.divider()
    st.markdown("### ğŸ¤– ì‚¬ìš© íŒ")
    st.markdown("""
    - **íŒŒì¼ ì—…ë¡œë“œ** ì‹œ ìë™ ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.
    - ë¶„ì„ í›„ **ì±„íŒ…ì°½**ì— ì´ë ‡ê²Œ ë¬¼ì–´ë³´ì„¸ìš”.
        - "í‰ê°€ ê¸°ì¤€í‘œ ë³´ì—¬ì¤˜"
        - "ì„œë²„ êµ¬ì¶• ìš”êµ¬ì‚¬í•­ì´ ë­ì•¼?"
        - "ì œì¶œ ì„œë¥˜ ëª©ë¡ ì •ë¦¬í•´ì¤˜"
    """)

if uploaded_file is not None:
    # 1. íŒŒì¼ ì´ë¦„ì— ì›ë˜ ì´ë¦„ì„ ë¶™ì—¬ì„œ ê³ ìœ í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
    temp_pdf_path = f"temp_rfp_{uploaded_file.name}"
    
    with open(temp_pdf_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    
    # 2. íŒŒì¼ì´ ë°”ë€Œì—ˆëŠ”ì§€ ì²´í¬ (ì—†ìœ¼ë©´ ìƒì„±, ë‹¤ë¥´ë©´ ì´ˆê¸°í™”)
    if "last_uploaded_file" not in st.session_state or st.session_state["last_uploaded_file"] != uploaded_file.name:
        st.session_state["last_uploaded_file"] = uploaded_file.name
        st.session_state["messages"] = []      # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
        if "retriever" in st.session_state:
            del st.session_state["retriever"]  # ê¸°ì¡´ í•™ìŠµ ë‚´ìš© ì‚­ì œ

    try:
        # 3. ë¶„ì„ ì‹œì‘ (retrieverê°€ ì—†ì„ ë•Œë§Œ ì‹¤í–‰)
        if "retriever" not in st.session_state:
            with st.spinner(f"ğŸ” '{project_name}' ì œì•ˆìš”ì²­ì„œë¥¼ ê¼¼ê¼¼íˆ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                
                retriever, docs = process_pdf(temp_pdf_path)
                st.session_state["retriever"] = retriever
                
                # ë¶„ì„ ê²°ê³¼ ìƒì„±
                analysis_result = analyze_rfp(docs, project_name)
                
                # [ì•ˆì „ì¥ì¹˜] í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ì—¬ê¸°ì„œë„ append í•˜ê¸° ì „ì— ë¦¬ìŠ¤íŠ¸ í™•ì¸
                if "messages" not in st.session_state:
                    st.session_state["messages"] = []

                st.session_state["messages"].append(
                    AIMessage(content=f"**[{project_name}]** ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. ğŸ‘‡\n\n{analysis_result}")
                )
                
        st.success("ë¶„ì„ ì™„ë£Œ! ì±„íŒ…ìœ¼ë¡œ ìƒì„¸ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”.")
        
    except Exception as e:
        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()

# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
for msg in st.session_state["messages"]:
    if isinstance(msg, HumanMessage):
        st.chat_message("user").write(msg.content)
    elif isinstance(msg, AIMessage):
        st.chat_message("assistant", avatar="ğŸ‘¨â€ğŸ’¼").write(msg.content) # ì•„ë°”íƒ€ë¥¼ ì§ì¥ì¸ìœ¼ë¡œ ë³€ê²½

# ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬
if user_input := st.chat_input("ì˜ˆ: ê¸°ìˆ  í‰ê°€ í•­ëª©ì´ ë­ì•¼? / íˆ¬ì… ì¸ë ¥ ì¡°ê±´ì´ ìˆì–´?"):
    st.chat_message("user").write(user_input)
    st.session_state["messages"].append(HumanMessage(content=user_input))

    with st.chat_message("assistant", avatar="ğŸ‘¨â€ğŸ’¼"):
        message_placeholder = st.empty()
        full_response = ""
        
        with st.spinner("ì œì•ˆìš”ì²­ì„œì—ì„œ ê´€ë ¨ ì¡°í•­ ì°¾ëŠ” ì¤‘... ğŸ“‘"):
            retriever = st.session_state["retriever"]
            retrieved_docs = retriever.invoke(user_input)
            context = "\n\n".join([doc.page_content for doc in retrieved_docs])
        
        # ì±„íŒ…ìš© í”„ë¡¬í”„íŠ¸ (ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜)
        prompt_template = ChatPromptTemplate.from_template(f"""
        ë„ˆëŠ” ì œì•ˆì„œ ì‘ì„± ì „ë¬¸ê°€(PM)ì•¼. 
        ì‚¬ìš©ìëŠ” ì´ ì‚¬ì—…ì„ ìˆ˜ì£¼í•˜ê³  ì‹¶ì–´ í•˜ëŠ” ì œì•ˆ ë‹´ë‹¹ìì•¼.
        
        [ì§€ì‹œ ì‚¬í•­]:
        1. [ì°¸ê³  ìë£Œ]ì¸ ì œì•ˆìš”ì²­ì„œ ë‚´ìš©ì— ê·¼ê±°í•´ì„œ íŒ©íŠ¸ ìœ„ì£¼ë¡œ ë‹µë³€í•´.
        2. ì œì•ˆì„œ ì‘ì„±ì— ë„ì›€ì´ ë˜ëŠ” íŒ(ì „ëµ)ì„ í•œ ì¤„ì”© ë§ë¶™ì—¬ì£¼ë©´ ë” ì¢‹ì•„.
        3. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "RFPì— ëª…ì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"ë¼ê³  ì†”ì§í•˜ê²Œ ë§í•´.

        [ì°¸ê³  ìë£Œ]:
        {{context}}
        
        ë‹´ë‹¹ì ì§ˆë¬¸: {{input}}
        """)
        
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.3)
        chain = prompt_template | llm
        
        chunks = chain.stream({"context": context, "input": user_input})
        
        for chunk in chunks:
            if chunk.content:
                full_response += chunk.content
                message_placeholder.markdown(full_response + "â–Œ")
                time.sleep(0.03)
        
        message_placeholder.markdown(full_response)
    
    st.session_state["messages"].append(AIMessage(content=full_response))