import streamlit as st
import os
import time
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate

# 1. í™˜ê²½ ì„¤ì •
load_dotenv()
st.set_page_config(page_title="ì½”ë”” ì„ ìƒë‹˜ (Live)", page_icon="ğŸ‘¨â€ğŸ«")
st.title("ğŸ‘¨â€ğŸ« ì‹¤ì‹œê°„ìœ¼ë¡œ ê°€ë¥´ì³ì£¼ëŠ” 'ì½”ë””'")

# 2. PDF ì²˜ë¦¬ ë° ë²¡í„° DB ìƒì„±
@st.cache_resource
def process_pdf(file_path):
    loader = PyPDFLoader(file_path)
    docs = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
    # í€´ì¦ˆ ìƒì„±ì„ ìœ„í•´ ê²€ìƒ‰ê¸°(retriever)ì™€ ì›ë³¸(docs) ë‘˜ ë‹¤ ë°˜í™˜
    return vectorstore.as_retriever(), docs

# 3. ìš”ì•½ ë° í€´ì¦ˆ ìƒì„±
def generate_summary_and_quiz(docs):
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7)
    
    # ì•ë¶€ë¶„ 3í˜ì´ì§€ë§Œ ì½ê¸°
    max_pages = 3
    context_text = "\n\n".join([doc.page_content for doc in docs[:max_pages]])

    prompt = f"""
    ë„ˆëŠ” ì´ˆë“±í•™ìƒë“¤ì˜ ëˆˆë†’ì´ì— ë§ì¶˜ ì¹œì ˆí•œ ì„ ìƒë‹˜ 'ì½”ë””'ì•¼. 
    ì•„ë˜ [êµì¬ ë‚´ìš©]ì„ ë³´ê³  ìˆ˜ì—…ì„ ì¤€ë¹„í•´ì¤˜.
    
    [êµì¬ ë‚´ìš©]:
    {context_text}
    
    [ìš”ì²­ ì‚¬í•­]:
    1. **ì˜¤ëŠ˜ì˜ í•™ìŠµ ëª©í‘œ**: í•µì‹¬ ì£¼ì œ 3ê°€ì§€ë¥¼ ë½‘ì•„ì„œ ìš”ì•½í•´ì¤˜.
    2. **ì¬ë¯¸ìˆëŠ” í€´ì¦ˆ**: ì•„ì´ë“¤ì´ í¥ë¯¸ë¥¼ ê°€ì§ˆë§Œí•œ ë‚´ìš©ìœ¼ë¡œ 3ì§€ ì„ ë‹¤í˜• í€´ì¦ˆë¥¼ 1ê°œë§Œ ë§Œë“¤ì–´ì¤˜.
    
    ì¶œë ¥ í˜•ì‹:
    ---
    ### ğŸ“ ì˜¤ëŠ˜ì˜ í•™ìŠµ ëª©í‘œ
    (ìš”ì•½)
    
    ### ğŸ§© íŒ í€´ì¦ˆ!
    (ë¬¸ì œ)
    1. (ë³´ê¸°)
    2. (ë³´ê¸°)
    3. (ë³´ê¸°)
    ---
    """
    response = llm.invoke(prompt)
    return response.content

# --- UI êµ¬ì„± ---
with st.sidebar:
    st.header("êµì¬ ì—…ë¡œë“œ ğŸ“¤")
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”", type=["pdf"])

if uploaded_file is not None:
    temp_pdf_path = "temp_lesson.pdf"
    with open(temp_pdf_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    
    try:
        if "retriever" not in st.session_state:
            with st.spinner("ì½”ë””ê°€ êµì¬ë¥¼ ì½ê³  ìˆ˜ì—… ì¤€ë¹„ ì¤‘... ğŸ“š"):
                retriever, docs = process_pdf(temp_pdf_path)
                st.session_state["retriever"] = retriever
                
                summary = generate_summary_and_quiz(docs)
                
                st.session_state["messages"] = [
                    AIMessage(content=f"ì, ìˆ˜ì—… ì‹œì‘í•´ë³¼ê¹Œìš”? ğŸ˜\n\n{summary}")
                ]
        st.success("ìˆ˜ì—… ì¤€ë¹„ ì™„ë£Œ!")
        
    except Exception as e:
        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()
else:
    if "retriever" in st.session_state:
        del st.session_state["retriever"]
        st.session_state["messages"] = []
    st.info("ğŸ‘ˆ ì™¼ìª½ì—ì„œ PDF êµì¬ë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    st.stop()

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for msg in st.session_state["messages"]:
    if isinstance(msg, HumanMessage):
        st.chat_message("user").write(msg.content)
    elif isinstance(msg, AIMessage):
        st.chat_message("assistant").write(msg.content)

# â˜… ì—¬ê¸°ê°€ í•µì‹¬: ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë° ìŠ¤íŠ¸ë¦¬ë°
if user_input := st.chat_input("ì§ˆë¬¸í•´ ë³´ì„¸ìš”!"):
    # 1. ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.chat_message("user").write(user_input)
    st.session_state["messages"].append(HumanMessage(content=user_input))

    # 2. AI ë‹µë³€ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
    with st.chat_message("assistant"):
        # ë¹ˆ ë°•ìŠ¤ë¥¼ ë¨¼ì € ë§Œë“¤ì–´ì„œ ì—¬ê¸°ë‹¤ê°€ ê¸€ìë¥¼ í•˜ë‚˜ì”© ì±„ìš¸ ê²ë‹ˆë‹¤.
        message_placeholder = st.empty()
        full_response = ""
        
        # (1) ê²€ìƒ‰ ë‹¨ê³„: ê²€ìƒ‰í•˜ëŠ” ë™ì•ˆì€ ìŠ¤í”¼ë„ˆë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
        with st.spinner("êµê³¼ì„œ ë’¤ì ì´ëŠ” ì¤‘... ğŸ“–"):
            retriever = st.session_state["retriever"]
            retrieved_docs = retriever.invoke(user_input)
            context = "\n\n".join([doc.page_content for doc in retrieved_docs])
        
        # (2) ìƒì„± ë‹¨ê³„: ê²€ìƒ‰ì´ ëë‚˜ë©´ ë°”ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
        prompt_template = ChatPromptTemplate.from_template("""
        ë„ˆëŠ” ì¹œì ˆí•œ ì„ ìƒë‹˜ 'ì½”ë””'ì•¼. 
        ì•„ë˜ [ì°¸ê³  ìë£Œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜.
        
        [ì°¸ê³  ìë£Œ]:
        {context}
        
        ì§ˆë¬¸: {input}
        """)
        
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7)
        chain = prompt_template | llm
        
        # â˜… ê°•ì œ ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„
        # chain.stream()ì—ì„œ ì¡°ê°(chunk)ì´ ë‚˜ì˜¬ ë•Œë§ˆë‹¤ í™”ë©´ì„ ê°±ì‹ í•©ë‹ˆë‹¤.
        chunks = chain.stream({"context": context, "input": user_input})
        
        for chunk in chunks:
            # contentê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬
            if chunk.content:
                full_response += chunk.content
                # â–Œ ë¬¸ìë¥¼ ë’¤ì— ë¶™ì—¬ì„œ ì»¤ì„œê°€ ê¹œë¹¡ì´ëŠ” ëŠë‚Œì„ ì¤ë‹ˆë‹¤.
                message_placeholder.markdown(full_response + "â–Œ")
                # ë„ˆë¬´ ë¹ ë¥´ë©´ ëˆˆì— ì•ˆ ë³´ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì•„ì£¼ ì°°ë‚˜ì˜ ë”œë ˆì´ (ì„ íƒì‚¬í•­)
                time.sleep(0.05)
        
        # ë‹¤ ëë‚˜ë©´ ì»¤ì„œ(â–Œ)ë¥¼ ì—†ì• ê³  ìµœì¢…ë³¸ í™•ì •
        message_placeholder.markdown(full_response)
    
    # 3. ì„¸ì…˜ì— ì €ì¥
    st.session_state["messages"].append(AIMessage(content=full_response))