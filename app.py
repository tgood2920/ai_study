import streamlit as st
import os
import time
import pandas as pd
import io
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate

# 1. í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
load_dotenv()
st.set_page_config(page_title="RFP ì…ì°° ë¶„ì„ & ìŠ¤í† ë¦¬ë³´ë“œ ìƒì„±ê¸°", page_icon="ğŸ“‘", layout="wide")

# [ì¤‘ìš”] ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (KeyError ë°©ì§€)
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "analysis_done" not in st.session_state:
    st.session_state["analysis_done"] = False

st.title("ğŸ“‘ RFP ë¶„ì„ & ì œì•ˆ ìŠ¤í† ë¦¬ë³´ë“œ ìƒì„±ê¸°")
st.markdown("""
ì…ì°° ê³µê³ (RFP)ë¥¼ ë¶„ì„í•˜ì—¬ **í•µì‹¬ ìš”ì•½**, **ë…ì†Œ ì¡°í•­ ì²´í¬**, ê·¸ë¦¬ê³  **ì œì•ˆì„œ ëª©ì°¨(Excel)**ê¹Œì§€ í•œ ë²ˆì— ìƒì„±í•©ë‹ˆë‹¤.
""")

# 2. PDF ì²˜ë¦¬ ë° ë²¡í„° DB ìƒì„± í•¨ìˆ˜
@st.cache_resource
def process_pdf(file_path):
    loader = PyPDFLoader(file_path)
    docs = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
    
    return vectorstore.as_retriever(), docs

# 3. RFP ë¶„ì„ í•¨ìˆ˜ (ìš”ì•½ ë° ë¦¬ìŠ¤í¬)
def analyze_rfp(docs, project_name):
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.3)
    
    # ì•ë¶€ë¶„(ê°œìš”)ê³¼ ë’·ë¶€ë¶„(í‰ê°€) ë“± ì£¼ìš” ë¶€ë¶„ ì°¸ì¡°
    # ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì• 10í˜ì´ì§€ì™€ ë’¤ 5í˜ì´ì§€ë¥¼ ì¡°í•©
    ref_docs = docs[:10] + docs[-5:]
    context_text = "\n\n".join([doc.page_content for doc in ref_docs])

    prompt = f"""
    ë„ˆëŠ” 20ë…„ ì°¨ ê³µê³µì‚¬ì—… ì œì•ˆ PMì´ì•¼.
    [ì œì•ˆìš”ì²­ì„œ(RFP)]ë¥¼ ë¶„ì„í•´ì„œ í•µì‹¬ì„ ì •ë¦¬í•´ì¤˜.
    
    [ì‚¬ì—…ëª…]: {project_name}
    [RFP ë‚´ìš© ì¼ë¶€]: {context_text}
    
    [ì¶œë ¥ ì–‘ì‹]:
    ## 1. ğŸ¯ ì‚¬ì—… ê°œìš”
    * **ì‚¬ì—… ëª©ì **: (í•œ ì¤„ ìš”ì•½)
    * **ì˜ˆì‚° ë° ê¸°ê°„**: (ì˜ˆì‚° / ê¸°ê°„)
    * **ì£¼ìš” ê³¼ì—…**: (í•µì‹¬ ìš”êµ¬ì‚¬í•­ 3ê°€ì§€)

    ## 2. âš ï¸ ë¦¬ìŠ¤í¬ ë° ì œì•½ì‚¬í•­ (ë…ì†Œì¡°í•­)
    * **ì…ì°° ìê²©**: (ì œí•œì‚¬í•­)
    * **ì¸ë ¥ ìš”ê±´**: (PMë“±ê¸‰, ìƒì£¼ ì—¬ë¶€ ë“±)
    * **í˜ë„í‹°/ì œì•½**: (ì§€ì²´ìƒê¸ˆë¥ , ê¸°ìˆ ì´ì „ ë“± íŠ¹ì´ì‚¬í•­)
    
    ## 3. ğŸ’¡ ì œì•ˆ ì „ëµ ê°€ì´ë“œ
    * ì´ ì‚¬ì—…ì„ ìˆ˜ì£¼í•˜ê¸° ìœ„í•´ ê°•ì¡°í•´ì•¼ í•  ì°¨ë³„í™” í¬ì¸íŠ¸ 3ê°€ì§€.
    """
    response = llm.invoke(prompt)
    return response.content

# 4. [New] ìŠ¤í† ë¦¬ë³´ë“œ(ì—‘ì…€) ë°ì´í„° ìƒì„± í•¨ìˆ˜
def generate_storyboard_data(docs, project_name):
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.5)
    
    # ëª©ì°¨ ìƒì„±ì„ ìœ„í•´ ë¬¸ì„œ ì „ë°˜ì ì¸ ë§¥ë½ í•„ìš”
    ref_docs = docs[:15] + docs[-5:]
    context_text = "\n\n".join([doc.page_content for doc in ref_docs])
    
    prompt = f"""
    ë„ˆëŠ” ì œì•ˆì„œ ì‘ì„±ì„ ì´ê´„í•˜ëŠ” ë©”ì¸ ê¸°íšìì•¼.
    RFP ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ìƒì„¸ ì œì•ˆ ëª©ì°¨(ìŠ¤í† ë¦¬ë³´ë“œ)**ë¥¼ ì—‘ì…€ë¡œ ë§Œë“¤ë ¤ê³  í•´.
    ì¼ë°˜ì ì¸ ê³µê³µ SI ì œì•ˆì„œ í‘œì¤€ ëª©ì°¨(ì „ëµ, ê°œìš”, ê¸°ìˆ , ê´€ë¦¬, ì§€ì›)ë¥¼ ë”°ë¥´ë˜, 
    RFPì˜ ìš”êµ¬ì‚¬í•­ê³¼ í‰ê°€í•­ëª©ì„ ì ì ˆí•œ ëª©ì°¨ì— ë°°ì¹˜í•´ì¤˜.

    [ì§€ì‹œì‚¬í•­]:
    1. **ë°˜ë“œì‹œ ì•„ë˜ CSV í˜•ì‹(íŒŒì´í”„ë¼ì¸ | êµ¬ë¶„)ìœ¼ë¡œë§Œ ì¶œë ¥í•´.** (ì‚¬ì¡± ë¶™ì´ì§€ ë§ˆ)
    2. 'í•µì‹¬ì‘ì„±ë‚´ìš©'ì—ëŠ” í•´ë‹¹ ëª©ì°¨ì— ë“¤ì–´ê°€ì•¼ í•  ì°¨ë³„í™” ì „ëµì´ë‚˜ í•„ìˆ˜ ë‚´ìš©ì„ ì ì–´.
    3. 'ì˜ˆìƒí˜ì´ì§€'ëŠ” ì „ì²´ 200í˜ì´ì§€ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ìš”ë„ì— ë”°ë¼ ë°°ë¶„í•´.
    
    [CSV ì¶œë ¥ í¬ë§·]:
    ëŒ€ëª©ì°¨|ì¤‘ëª©ì°¨|ì†Œëª©ì°¨|í•µì‹¬ì‘ì„±ë‚´ìš©(Key Message)|ê´€ë ¨ìš”êµ¬ì‚¬í•­ID|ì˜ˆìƒí˜ì´ì§€
    I. ì œì•ˆê°œìš”|1. ì œì•ˆë°°ê²½|1.1 ì¶”ì§„ë°°ê²½ ë° ëª©ì |ì‚¬ì—… ì´í•´ë„ ë° ê¸°ëŒ€íš¨ê³¼ ê°•ì¡°|REQ-001|2
    I. ì œì•ˆê°œìš”|2. ì‚¬ì—…ë²”ìœ„|2.1 ëª©í‘œì‹œìŠ¤í…œ êµ¬ì„±|To-Be ëª¨ë¸ ì•„í‚¤í…ì²˜ ì œì‹œ|REQ-002|3
    ... (ê³„ì† ì‘ì„±) ...
    """
    
    response = llm.invoke(prompt)
    return response.content

# --- UI êµ¬ì„± ---

with st.sidebar:
    st.header("ğŸ“‚ í”„ë¡œì íŠ¸ ì„¤ì •")
    project_name = st.text_input("ì‚¬ì—…ëª…", value="ì°¨ì„¸ëŒ€ ì •ë³´ì‹œìŠ¤í…œ êµ¬ì¶• ì‚¬ì—…")
    uploaded_file = st.file_uploader("RFP(PDF) ì—…ë¡œë“œ", type=["pdf"])
    
    st.divider()
    st.info("ğŸ’¡ PDFë¥¼ ì—…ë¡œë“œí•˜ë©´ ìë™ìœ¼ë¡œ ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.")

# ë©”ì¸ ë¡œì§
if uploaded_file is not None:
    # 1. íŒŒì¼ ì €ì¥ ë° ì„¸ì…˜ ê´€ë¦¬
    temp_pdf_path = f"temp_{uploaded_file.name}"
    with open(temp_pdf_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    
    # íŒŒì¼ ë³€ê²½ ê°ì§€ ë° ì´ˆê¸°í™”
    if "last_uploaded_file" not in st.session_state or st.session_state["last_uploaded_file"] != uploaded_file.name:
        st.session_state["last_uploaded_file"] = uploaded_file.name
        st.session_state["messages"] = []
        st.session_state["analysis_done"] = False
        if "retriever" in st.session_state:
            del st.session_state["retriever"]

    try:
        # 2. PDF ì²˜ë¦¬ (í•œ ë²ˆë§Œ ì‹¤í–‰)
        if "retriever" not in st.session_state:
            with st.spinner(f"ğŸ” '{project_name}' RFP ë¶„ì„ ì¤‘... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)"):
                retriever, docs = process_pdf(temp_pdf_path)
                st.session_state["retriever"] = retriever
                st.session_state["docs"] = docs # ì—‘ì…€ ìƒì„±ì„ ìœ„í•´ ì›ë³¸ ì €ì¥
                
                # 3. ê¸°ë³¸ ë¶„ì„ ìˆ˜í–‰
                analysis_result = analyze_rfp(docs, project_name)
                
                # ê²°ê³¼ ë©”ì‹œì§€ ì €ì¥
                welcome_msg = AIMessage(content=f"**[{project_name}]** ë¶„ì„ ì™„ë£Œ! ğŸš€\n\n{analysis_result}")
                st.session_state["messages"].append(welcome_msg)
                st.session_state["analysis_done"] = True

        # --- í™”ë©´ í‘œì‹œ ì˜ì—­ ---
        
        # (1) ì±„íŒ…ì°½ (ë¶„ì„ ê²°ê³¼ ë° ëŒ€í™”)
        for msg in st.session_state["messages"]:
            if isinstance(msg, HumanMessage):
                st.chat_message("user").write(msg.content)
            elif isinstance(msg, AIMessage):
                st.chat_message("assistant", avatar="ğŸ‘¨â€ğŸ’¼").write(msg.content)

        # (2) [New] ìŠ¤í† ë¦¬ë³´ë“œ ìƒì„± ë²„íŠ¼ (ë¶„ì„ ì™„ë£Œ ì‹œì—ë§Œ í‘œì‹œ)
        if st.session_state["analysis_done"]:
            st.divider()
            col1, col2 = st.columns([3, 1])
            with col1:
                st.markdown("### ğŸ“Š ì œì•ˆ ìŠ¤í† ë¦¬ë³´ë“œ(Excel) ë§Œë“¤ê¸°")
                st.markdown("RFP ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ **ìƒì„¸ ëª©ì°¨, í˜ì´ì§€ ê³„íš, í•µì‹¬ ì „ëµ**ì´ ë‹´ê¸´ ì—‘ì…€ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            
            with col2:
                generate_btn = st.button("ìŠ¤í† ë¦¬ë³´ë“œ ìƒì„± âœ¨")
            
            if generate_btn:
                with st.spinner("AIê°€ ì œì•ˆ ì „ëµì„ ì§œê³  ì—‘ì…€ì„ ë§Œë“œëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                    docs = st.session_state["docs"]
                    csv_data = generate_storyboard_data(docs, project_name)
                    
                    # ë°ì´í„° ì „ì²˜ë¦¬ (CSV íŒŒì‹±)
                    try:
                        valid_lines = [line for line in csv_data.split('\n') if '|' in line]
                        clean_csv = "\n".join(valid_lines)
                        
                        df = pd.read_csv(io.StringIO(clean_csv), sep="|")
                        
                        # ì—‘ì…€ ë³€í™˜ (ë©”ëª¨ë¦¬ ìƒì—ì„œ ì²˜ë¦¬)
                        output = io.BytesIO()
                        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                            df.to_excel(writer, index=False, sheet_name='ì œì•ˆëª©ì°¨_v1.0')
                            
                            # (ì„ íƒ) ì—‘ì…€ ìŠ¤íƒ€ì¼ë§
                            workbook = writer.book
                            worksheet = writer.sheets['ì œì•ˆëª©ì°¨_v1.0']
                            header_fmt = workbook.add_format({'bold': True, 'bg_color': '#D7E4BC', 'border': 1})
                            for col_num, value in enumerate(df.columns.values):
                                worksheet.write(0, col_num, value, header_fmt)
                                worksheet.set_column(col_num, col_num, 20) # ë„ˆë¹„ ì¡°ì ˆ

                        excel_data = output.getvalue()
                        
                        st.success("ìƒì„± ì™„ë£Œ! ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                        st.dataframe(df) # ë¯¸ë¦¬ë³´ê¸°
                        
                        st.download_button(
                            label="ğŸ“¥ ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (.xlsx)",
                            data=excel_data,
                            file_name=f"{project_name}_ì œì•ˆìŠ¤í† ë¦¬ë³´ë“œ.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )
                    except Exception as e:
                        st.error("ì—‘ì…€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. AI ì‘ë‹µì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                        with st.expander("AI ì›ë³¸ ì‘ë‹µ ë³´ê¸°"):
                            st.write(csv_data)

        # (3) ì‚¬ìš©ì ì¶”ê°€ ì§ˆë¬¸ ì…ë ¥
        if user_input := st.chat_input("RFPì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš” (ì˜ˆ: í‰ê°€ ë°°ì ì´ ì–´ë–»ê²Œ ë¼?)"):
            st.chat_message("user").write(user_input)
            st.session_state["messages"].append(HumanMessage(content=user_input))

            with st.chat_message("assistant", avatar="ğŸ‘¨â€ğŸ’¼"):
                message_placeholder = st.empty()
                full_response = ""
                
                with st.spinner("RFP í™•ì¸ ì¤‘..."):
                    retriever = st.session_state["retriever"]
                    retrieved_docs = retriever.invoke(user_input)
                    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
                
                # ì±„íŒ…ìš© í”„ë¡¬í”„íŠ¸
                prompt_template = ChatPromptTemplate.from_template(f"""
                ë„ˆëŠ” ì œì•ˆ PMì´ì•¼. RFP ë‚´ìš©ì„ ê·¼ê±°ë¡œ ë‹µë³€í•´.
                [ì°¸ê³  ìë£Œ]: {{context}}
                [ì§ˆë¬¸]: {{input}}
                """)
                
                llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.3)
                chain = prompt_template | llm
                chunks = chain.stream({"context": context, "input": user_input})
                
                for chunk in chunks:
                    if chunk.content:
                        full_response += chunk.content
                        message_placeholder.markdown(full_response + "â–Œ")
                        time.sleep(0.03)
                message_placeholder.markdown(full_response)
            
            st.session_state["messages"].append(AIMessage(content=full_response))
            
    except Exception as e:
        st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

else:
    # íŒŒì¼ ì—†ì„ ë•Œ ì•ˆë‚´
    st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ PDF ì œì•ˆìš”ì²­ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")