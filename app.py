import streamlit as st
import os
import time
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate

# 1. í™˜ê²½ ì„¤ì •
load_dotenv()
st.set_page_config(page_title="ë§ŒëŠ¥ AI íŠœí„°", page_icon="ğŸ“")
st.title("ğŸ“ ë¬´ì—‡ì´ë“  ê°€ë¥´ì³ ë“œë ¤ìš”!")

# 2. PDF ì²˜ë¦¬ ë° ë²¡í„° DB ìƒì„±
@st.cache_resource
def process_pdf(file_path):
    loader = PyPDFLoader(file_path)
    docs = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
    return vectorstore.as_retriever(), docs

# 3. ìš”ì•½ ë° í€´ì¦ˆ ìƒì„± (ë²”ìš© ë²„ì „)
def generate_summary_and_quiz(docs, topic, level):
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7)
    
    max_pages = 3
    context_text = "\n\n".join([doc.page_content for doc in docs[:max_pages]])

    # í”„ë¡¬í”„íŠ¸ì— 'ì£¼ì œ(topic)'ì™€ 'ëŒ€ìƒ(level)'ì„ ë™ì ìœ¼ë¡œ ë„£ìŠµë‹ˆë‹¤.
    prompt = f"""
    ë„ˆëŠ” ì§€ê¸ˆë¶€í„° ìœ ëŠ¥í•œ '{topic}' ì„ ìƒë‹˜ì´ì•¼.
    ë‚´ í•™ìƒì˜ ìˆ˜ì¤€ì€ '{level}'ì´ì•¼. ì´ ìˆ˜ì¤€ì— ë”± ë§ì¶°ì„œ ì„¤ëª…í•´ì•¼ í•´.
    
    ì•„ë˜ [êµì¬ ë‚´ìš©]ì„ ë³´ê³  ìˆ˜ì—…ì„ ì¤€ë¹„í•´ì¤˜.
    
    [êµì¬ ë‚´ìš©]:
    {context_text}
    
    [ìš”ì²­ ì‚¬í•­]:
    1. **ì˜¤ëŠ˜ì˜ í•µì‹¬ ìš”ì•½**: ì´ êµì¬ì˜ í•µì‹¬ ë‚´ìš©ì„ 3ê°€ì§€ë¡œ ìš”ì•½í•´ì¤˜.
    2. **ë§ì¶¤í˜• í€´ì¦ˆ**: '{level}' ìˆ˜ì¤€ì— ë§ëŠ” 3ì§€ ì„ ë‹¤í˜• í€´ì¦ˆë¥¼ 1ê°œ ë§Œë“¤ì–´ì¤˜.
    
    ì¶œë ¥ í˜•ì‹:
    ---
    ### ğŸ“ {topic} í•µì‹¬ ìš”ì•½ ({level}ìš©)
    (ìš”ì•½ ë‚´ìš©)
    
    ### ğŸ§© íŒ í€´ì¦ˆ!
    (ë¬¸ì œ)
    1. (ë³´ê¸°)
    2. (ë³´ê¸°)
    3. (ë³´ê¸°)
    ---
    """
    response = llm.invoke(prompt)
    return response.content

# --- UI êµ¬ì„± ---
with st.sidebar:
    st.header("í•™ìŠµ ì„¤ì • âš™ï¸")
    
    # [New] ê³¼ëª©ëª…ê³¼ ë‚œì´ë„ë¥¼ ì‚¬ìš©ìê°€ ì§ì ‘ ê³ ë¥´ê²Œ í•©ë‹ˆë‹¤.
    topic = st.text_input("ê³µë¶€í•  ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”", value="ì¼ë°˜ ìƒì‹")
    level = st.selectbox("í•™ìŠµ ë‚œì´ë„(ëŒ€ìƒ)", ["ì´ˆë“±í•™ìƒ", "ì¤‘ê³ ë“±í•™ìƒ", "ëŒ€í•™ìƒ/ì „ë¬¸ê°€", "ì¼ë°˜ì¸"])
    
    st.divider()
    st.header("êµì¬ ì—…ë¡œë“œ ğŸ“¤")
    uploaded_file = st.file_uploader("PDF êµì¬ë¥¼ ì˜¬ë ¤ì£¼ì„¸ìš”", type=["pdf"])

if uploaded_file is not None:
    temp_pdf_path = "temp_lesson.pdf"
    with open(temp_pdf_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    
    try:
        # íŒŒì¼ì´ ë°”ë€Œê±°ë‚˜ ì„¤ì •ì´ ë°”ë€Œë©´ ë¦¬ì…‹í•˜ê¸° ìœ„í•´ session_state ì²´í¬ ë¡œì§ì„ ë‹¨ìˆœí™”í–ˆìŠµë‹ˆë‹¤.
        if "retriever" not in st.session_state or st.sidebar.button("ì„¤ì • ì ìš© ë° ë‹¤ì‹œ í•™ìŠµ"):
            with st.spinner(f"AIê°€ '{topic}' ê³¼ëª©ì„ '{level}' ìˆ˜ì¤€ìœ¼ë¡œ ê³µë¶€í•˜ëŠ” ì¤‘... ğŸ“š"):
                retriever, docs = process_pdf(temp_pdf_path)
                st.session_state["retriever"] = retriever
                
                # ìš”ì•½ë³¸ ìƒì„± ì‹œ ì„¤ì •ê°’ ì „ë‹¬
                summary = generate_summary_and_quiz(docs, topic, level)
                
                st.session_state["messages"] = [
                    AIMessage(content=f"ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì˜¤ëŠ˜ ì—¬ëŸ¬ë¶„ì˜ **{topic}** ì„ ìƒë‹˜ì…ë‹ˆë‹¤.\n**{level}** ëˆˆë†’ì´ì— ë§ì¶° ìˆ˜ì—…í• ê²Œìš”! ğŸ˜\n\n{summary}")
                ]
        st.success("ì¤€ë¹„ ì™„ë£Œ!")
        
    except Exception as e:
        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()
else:
    # íŒŒì¼ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
    for key in ["retriever", "messages"]:
        if key in st.session_state:
            del st.session_state[key]
    st.info("ğŸ‘ˆ ì™¼ìª½ì—ì„œ ì£¼ì œë¥¼ ì ê³  PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    st.stop()

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for msg in st.session_state["messages"]:
    if isinstance(msg, HumanMessage):
        st.chat_message("user").write(msg.content)
    elif isinstance(msg, AIMessage):
        st.chat_message("assistant").write(msg.content)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_input := st.chat_input("ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!"):
    st.chat_message("user").write(user_input)
    st.session_state["messages"].append(HumanMessage(content=user_input))

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        with st.spinner("êµì¬ ë‚´ìš© ì°¾ì•„ë³´ëŠ” ì¤‘..."):
            retriever = st.session_state["retriever"]
            retrieved_docs = retriever.invoke(user_input)
            context = "\n\n".join([doc.page_content for doc in retrieved_docs])
        
        # ëŒ€í™” ê¸°ë¡ (ê¸°ì–µë ¥)
        chat_history = []
        for msg in st.session_state["messages"][-3:]: 
            role = "AI ì„ ìƒë‹˜" if isinstance(msg, AIMessage) else "í•™ìƒ"
            chat_history.append(f"{role}: {msg.content}")
        history_text = "\n".join(chat_history)

        # [New] í”„ë¡¬í”„íŠ¸ì—ë„ topicê³¼ levelì„ ì£¼ì…í•˜ì—¬ í˜ë¥´ì†Œë‚˜ ìœ ì§€
        prompt_template = ChatPromptTemplate.from_template(f"""
        ë„ˆëŠ” ìœ ëŠ¥í•˜ê³  ì¹œì ˆí•œ '{topic}' ì„ ìƒë‹˜ì´ì•¼.
        í•™ìŠµìëŠ” '{level}' ìˆ˜ì¤€ì´ì•¼. ì–´ë ¤ìš´ ë§ ì“°ì§€ ë§ê³  ëˆˆë†’ì´ì— ë§ì¶°ì„œ ì„¤ëª…í•´.
        
        [ì§€ì‹œ ì‚¬í•­]:
        1. [ëŒ€í™” ê¸°ë¡]ì„ ì°¸ê³ í•´ì„œ ë¬¸ë§¥ì„ ì´ì–´ê°€.
        2. ë°˜ë“œì‹œ [ì°¸ê³  ìë£Œ]ì— ê¸°ë°˜í•´ì„œ ëŒ€ë‹µí•´. ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•´.
        3. ì„¤ëª…ì€ ëª…í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ.

        [ëŒ€í™” ê¸°ë¡]:
        {{history}}

        [ì°¸ê³  ìë£Œ]:
        {{context}}
        
        ì§ˆë¬¸: {{input}}
        """)
        
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7)
        chain = prompt_template | llm
        
        chunks = chain.stream({
            "history": history_text, 
            "context": context, 
            "input": user_input
        })
        
        for chunk in chunks:
            if chunk.content:
                full_response += chunk.content
                message_placeholder.markdown(full_response + "â–Œ")
                time.sleep(0.03)
        
        message_placeholder.markdown(full_response)
    
    st.session_state["messages"].append(AIMessage(content=full_response))